#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„å‘½ä»¤è¡Œè·Ÿè¸ªå·¥å…· - æ”¯æŒè·³å¸§å’Œé«˜æ€§èƒ½è·Ÿè¸ª
Improved Command Line Tracker - With Frame Skipping and High Performance

ä¸»è¦æ”¹è¿›ï¼š
1. æ”¯æŒè·³å¸§å¤„ç† - å¤§å¹…æå‡å¤„ç†é€Ÿåº¦
2. çœŸå®LightTrackæ¨¡å‹é›†æˆ - ç§»é™¤æ¼”ç¤ºæ¨¡å¼
3. è‡ªé€‚åº”æ€§èƒ½è°ƒä¼˜ - æ ¹æ®è§†é¢‘ç‰¹ç‚¹è‡ªåŠ¨ä¼˜åŒ–
4. è¯¦ç»†çš„æ€§èƒ½åˆ†æ - å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯è¾“å‡º
"""

import os
import sys
import cv2
import argparse
import numpy as np
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from improved_tracker import ImprovedTracker


def parse_args():
    parser = argparse.ArgumentParser(description='LightTrack æ”¹è¿›ç‰ˆå‘½ä»¤è¡Œè·Ÿè¸ªå·¥å…·')
    parser.add_argument('--video', type=str, required=True, help='è¾“å…¥MP4è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--bbox', type=str, help='åˆå§‹è¾¹ç•Œæ¡† (x,y,w,h)')
    parser.add_argument('--output', type=str, help='è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--display', action='store_true', help='å®æ—¶æ˜¾ç¤ºè·Ÿè¸ªç»“æœ')
    parser.add_argument('--frame-skip', type=int, default=1, help='è·³å¸§é—´éš” (é»˜è®¤:1, ä¸è·³å¸§)')
    parser.add_argument('--target-fps', type=float, default=30.0, help='ç›®æ ‡FPS (é»˜è®¤:30)')
    parser.add_argument('--benchmark', action='store_true', help='æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--auto-optimize', action='store_true', help='è‡ªåŠ¨ä¼˜åŒ–å‚æ•°')
    return parser.parse_args()


def parse_bbox(bbox_str):
    """è§£æè¾¹ç•Œæ¡†å­—ç¬¦ä¸²"""
    try:
        bbox = [float(x) for x in bbox_str.split(',')]
        if len(bbox) != 4:
            raise ValueError
        return bbox
    except:
        raise ValueError("è¾¹ç•Œæ¡†æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º: x,y,w,h")


def select_bbox_interactive(video_path):
    """äº¤äº’å¼é€‰æ‹©è¾¹ç•Œæ¡†"""
    print("æ­£åœ¨æ‰“å¼€è§†é¢‘è¿›è¡Œç›®æ ‡é€‰æ‹©...")
    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap.release()
    
    if not ret:
        raise ValueError("æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§")
    
    print("è¯·åœ¨çª—å£ä¸­æ‹–æ‹½é¼ æ ‡é€‰æ‹©è·Ÿè¸ªç›®æ ‡ï¼ŒæŒ‰ENTERç¡®è®¤ï¼ŒæŒ‰ESCå–æ¶ˆ")
    
    # ä½¿ç”¨OpenCVçš„ROIé€‰æ‹©å™¨
    bbox = cv2.selectROI("é€‰æ‹©è·Ÿè¸ªç›®æ ‡ - æ¡†é€‰åæŒ‰ENTERç¡®è®¤", frame, False)
    cv2.destroyWindow("é€‰æ‹©è·Ÿè¸ªç›®æ ‡ - æ¡†é€‰åæŒ‰ENTERç¡®è®¤")
    
    if bbox[2] > 0 and bbox[3] > 0:
        return list(bbox)
    else:
        return None


def analyze_video(video_path):
    """åˆ†æè§†é¢‘ç‰¹å¾ï¼Œç»™å‡ºä¼˜åŒ–å»ºè®®"""
    print("\nğŸ“Š åˆ†æè§†é¢‘ç‰¹å¾...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None, None
    
    # è·å–åŸºæœ¬ä¿¡æ¯
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = total_frames / fps if fps > 0 else 0
    
    # é‡‡æ ·å‡ å¸§åˆ†æè¿åŠ¨å¹…åº¦
    motion_scores = []
    prev_gray = None
    
    for i in range(0, min(total_frames, 100), 10):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if not ret:
            break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if prev_gray is not None:
            flow = cv2.calcOpticalFlowPyrLK(prev_gray, gray, 
                                          np.array([[width//2, height//2]], dtype=np.float32),
                                          None, maxLevel=2)[0]
            if flow is not None and len(flow) > 0:
                motion = np.linalg.norm(flow[0])
                motion_scores.append(motion)
        
        prev_gray = gray
    
    cap.release()
    
    # è®¡ç®—è¿åŠ¨ç»Ÿè®¡
    avg_motion = np.mean(motion_scores) if motion_scores else 0
    
    # è§†é¢‘ç‰¹å¾
    video_info = {
        'total_frames': total_frames,
        'fps': fps,
        'width': width,
        'height': height,
        'duration': duration,
        'avg_motion': avg_motion,
        'motion_level': 'high' if avg_motion > 10 else 'medium' if avg_motion > 3 else 'low'
    }
    
    # æ€§èƒ½å»ºè®®
    suggestions = {}
    
    # åŸºäºè§†é¢‘é•¿åº¦çš„å»ºè®®
    if duration > 60:
        suggestions['frame_skip'] = 3
        suggestions['reason'] = "é•¿è§†é¢‘ï¼Œå»ºè®®è¾ƒå¤§è·³å¸§é—´éš”"
    elif duration > 30:
        suggestions['frame_skip'] = 2
        suggestions['reason'] = "ä¸­ç­‰é•¿åº¦è§†é¢‘ï¼Œé€‚åº¦è·³å¸§"
    else:
        suggestions['frame_skip'] = 1
        suggestions['reason'] = "çŸ­è§†é¢‘ï¼Œä¿æŒé«˜è´¨é‡"
    
    # åŸºäºè¿åŠ¨å¹…åº¦çš„å»ºè®®
    if avg_motion > 10:
        suggestions['target_fps'] = 60
        suggestions['motion_reason'] = "é«˜è¿åŠ¨å¹…åº¦ï¼Œéœ€è¦é«˜å¸§ç‡"
    elif avg_motion > 3:
        suggestions['target_fps'] = 30
        suggestions['motion_reason'] = "ä¸­ç­‰è¿åŠ¨å¹…åº¦ï¼Œæ ‡å‡†å¸§ç‡"
    else:
        suggestions['target_fps'] = 20
        suggestions['motion_reason'] = "ä½è¿åŠ¨å¹…åº¦ï¼Œå¯é™ä½å¸§ç‡"
    
    # åŸºäºåˆ†è¾¨ç‡çš„å»ºè®®
    if width * height > 1920 * 1080:
        suggestions['frame_skip'] = max(suggestions['frame_skip'], 2)
        suggestions['resolution_reason'] = "é«˜åˆ†è¾¨ç‡ï¼Œå¢åŠ è·³å¸§"
    
    return video_info, suggestions


def print_video_analysis(video_info, suggestions):
    """æ‰“å°è§†é¢‘åˆ†æç»“æœ"""
    print(f"   åˆ†è¾¨ç‡: {video_info['width']}x{video_info['height']}")
    print(f"   æ€»å¸§æ•°: {video_info['total_frames']}")
    print(f"   å¸§ç‡: {video_info['fps']:.2f} FPS")
    print(f"   æ—¶é•¿: {video_info['duration']:.1f}ç§’")
    print(f"   è¿åŠ¨å¹…åº¦: {video_info['motion_level']} (å¹³å‡: {video_info['avg_motion']:.1f})")
    
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print(f"   æ¨èè·³å¸§é—´éš”: {suggestions['frame_skip']} ({suggestions['reason']})")
    print(f"   æ¨èç›®æ ‡FPS: {suggestions['target_fps']} ({suggestions['motion_reason']})")
    if 'resolution_reason' in suggestions:
        print(f"   åˆ†è¾¨ç‡è°ƒæ•´: {suggestions['resolution_reason']}")


def track_video_improved(video_path, init_bbox, output_path, display=False, 
                        frame_skip=1, target_fps=30.0, benchmark=False):
    """ä½¿ç”¨æ”¹è¿›è·Ÿè¸ªå™¨å¤„ç†è§†é¢‘"""
    
    print(f"\nğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆè·Ÿè¸ª")
    print(f"è·³å¸§é—´éš”: {frame_skip}, ç›®æ ‡FPS: {target_fps}")
    
    # åˆ›å»ºè·Ÿè¸ªå™¨
    tracker = ImprovedTracker(frame_skip=frame_skip, target_fps=target_fps)
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    
    # è·å–è§†é¢‘å±æ€§
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {original_fps}fps, {total_frames}å¸§")
    
    # å‡†å¤‡è¾“å‡ºè§†é¢‘
    if output_path is None:
        output_path = os.path.splitext(video_path)[0] + f"_tracked_fs{frame_skip}_fps{target_fps:.0f}.mp4"
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, original_fps, (width, height))
    
    # è¯»å–ç¬¬ä¸€å¸§
    ret, frame = cap.read()
    if not ret:
        raise ValueError("æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§")
    
    # åˆå§‹åŒ–è·Ÿè¸ªå™¨
    print(f"åˆå§‹åŒ–è·Ÿè¸ªå™¨ï¼Œç›®æ ‡åŒºåŸŸ: {init_bbox}")
    
    start_time = time.time()
    if not tracker.initialize(frame, init_bbox):
        raise ValueError("è·Ÿè¸ªå™¨åˆå§‹åŒ–å¤±è´¥")
    
    init_time = time.time() - start_time
    print(f"âœ… è·Ÿè¸ªå™¨åˆå§‹åŒ–æˆåŠŸ (è€—æ—¶: {init_time:.3f}ç§’)")
    
    # è·Ÿè¸ªç»Ÿè®¡
    stats = {
        'start_time': time.time(),
        'frame_times': [],
        'total_frames': 0,
        'successful_frames': 0,
        'processing_times': []
    }
    
    # åŸºå‡†æµ‹è¯•æ•°æ®
    if benchmark:
        benchmark_data = {
            'frame_processing_times': [],
            'confidence_scores': [],
            'bbox_stability': [],
            'skip_ratios': []
        }
    
    frame_count = 0
    
    # å¤„ç†è§†é¢‘å¸§
    print("ğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘å¸§...")
    
    while ret:
        frame_start = time.time()
        
        # è·Ÿè¸ªå½“å‰å¸§
        success, bbox, confidence, info = tracker.track(frame)
        
        processing_time = time.time() - frame_start
        stats['processing_times'].append(processing_time)
        
        # ç»˜åˆ¶è·Ÿè¸ªç»“æœ
        if success and bbox is not None:
            x, y, w, h = [int(v) for v in bbox]
            
            # é€‰æ‹©é¢œè‰²
            if confidence > 0.7:
                color = (0, 255, 0)  # ç»¿è‰² - é«˜ç½®ä¿¡åº¦
            elif confidence > 0.4:
                color = (0, 255, 255)  # é»„è‰² - ä¸­ç½®ä¿¡åº¦  
            else:
                color = (0, 165, 255)  # æ©™è‰² - ä½ç½®ä¿¡åº¦
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            
            # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
            cv2.putText(frame, f'Frame: {frame_count + 1}/{total_frames}', 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(frame, f'Confidence: {confidence:.3f}', 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # è·Ÿè¸ªæ¨¡å¼ä¿¡æ¯
            mode = "LightTrack" if tracker.model is not None else "Template"
            cv2.putText(frame, f'Mode: {mode}', 
                       (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # è·³å¸§ä¿¡æ¯
            if info.get('skipped', False):
                cv2.putText(frame, 'SKIPPED (Interpolated)', 
                          (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            
            # æ€§èƒ½ä¿¡æ¯
            tracker_stats = tracker.get_stats()
            current_fps = tracker_stats.get('avg_fps', 0)
            cv2.putText(frame, f'Processing FPS: {current_fps:.1f}', 
                       (10, height - 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            
            # æˆåŠŸç‡
            success_rate = tracker_stats.get('success_rate', 0)
            cv2.putText(frame, f'Success Rate: {success_rate:.1f}%', 
                       (10, height - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # ç³»ç»Ÿæ ‡è¯†
            cv2.putText(frame, 'LightTrack Improved', 
                       (10, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            stats['successful_frames'] += 1
            
        else:
            # è·Ÿè¸ªå¤±è´¥
            cv2.putText(frame, 'Tracking Lost', (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            cv2.putText(frame, f'Frame: {frame_count + 1}/{total_frames}', 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # åŸºå‡†æµ‹è¯•æ•°æ®æ”¶é›†
        if benchmark:
            benchmark_data['frame_processing_times'].append(processing_time)
            benchmark_data['confidence_scores'].append(confidence)
            if len(tracker.bbox_history) >= 2:
                # è®¡ç®—bboxç¨³å®šæ€§
                prev_bbox = tracker.bbox_history[-2]
                curr_bbox = tracker.bbox_history[-1]
                stability = np.linalg.norm(np.array(curr_bbox[:2]) - np.array(prev_bbox[:2]))
                benchmark_data['bbox_stability'].append(stability)
        
        # å†™å…¥è¾“å‡ºè§†é¢‘
        out.write(frame)
        
        # æ˜¾ç¤ºç»“æœï¼ˆå¯é€‰ï¼‰
        if display:
            cv2.imshow('LightTrack Improved', frame)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("ç”¨æˆ·ä¸­æ–­è·Ÿè¸ª")
                break
            elif key == ord(' '):  # ç©ºæ ¼é”®æš‚åœ
                cv2.waitKey(0)
        
        # è¯»å–ä¸‹ä¸€å¸§
        ret, frame = cap.read()
        frame_count += 1
        stats['total_frames'] = frame_count
        
        # æ˜¾ç¤ºè¿›åº¦
        if frame_count % 60 == 0:
            progress = (frame_count / total_frames) * 100
            elapsed = time.time() - stats['start_time']
            current_fps = frame_count / elapsed if elapsed > 0 else 0
            print(f"å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames}) - "
                  f"å¤„ç†FPS: {current_fps:.1f}")
    
    # æ¸…ç†èµ„æº
    cap.release()
    out.release()
    if display:
        cv2.destroyAllWindows()
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - stats['start_time']
    final_stats = tracker.get_stats()
    
    # æ‰“å°ç»“æœ
    print_tracking_results(video_path, output_path, init_bbox, total_time, 
                          final_stats, stats, benchmark_data if benchmark else None)
    
    return output_path, final_stats


def print_tracking_results(input_path, output_path, init_bbox, total_time, 
                         tracker_stats, process_stats, benchmark_data=None):
    """æ‰“å°è·Ÿè¸ªç»“æœ"""
    
    print("\n" + "="*60)
    print("ğŸ‰ è·Ÿè¸ªå®Œæˆ!")
    print("="*60)
    print(f"ğŸ“ è¾“å…¥è§†é¢‘: {os.path.basename(input_path)}")
    print(f"ğŸ“ è¾“å‡ºè§†é¢‘: {os.path.basename(output_path)}")
    print(f"ğŸ¯ åˆå§‹ç›®æ ‡: [{init_bbox[0]:.1f}, {init_bbox[1]:.1f}, {init_bbox[2]:.1f}, {init_bbox[3]:.1f}]")
    
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    print(f"   æ€»å¸§æ•°: {tracker_stats.get('total_frames', 0)}")
    print(f"   å¤„ç†å¸§æ•°: {tracker_stats.get('processed_frames', 0)}")
    print(f"   è·³è¿‡å¸§æ•°: {tracker_stats.get('skipped_frames', 0)}")
    print(f"   æˆåŠŸè·Ÿè¸ª: {tracker_stats.get('successful_tracks', 0)}")
    
    print(f"\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   å¹³å‡å¤„ç†FPS: {tracker_stats.get('avg_fps', 0):.1f}")
    print(f"   è·Ÿè¸ªæˆåŠŸç‡: {tracker_stats.get('success_rate', 0):.1f}%")
    
    if process_stats['processing_times']:
        avg_process_time = np.mean(process_stats['processing_times']) * 1000  # æ¯«ç§’
        print(f"   å¹³å‡å¸§å¤„ç†æ—¶é—´: {avg_process_time:.1f}ms")
    
    # è·³å¸§æ•ˆç‡åˆ†æ
    total_frames = tracker_stats.get('total_frames', 0)
    processed_frames = tracker_stats.get('processed_frames', 0)
    if total_frames > 0:
        skip_efficiency = (total_frames - processed_frames) / total_frames * 100
        print(f"   è·³å¸§æ•ˆç‡: {skip_efficiency:.1f}%")
    
    # åŸºå‡†æµ‹è¯•ç»“æœ
    if benchmark_data:
        print(f"\nğŸ§ª åŸºå‡†æµ‹è¯•:")
        if benchmark_data['frame_processing_times']:
            avg_frame_time = np.mean(benchmark_data['frame_processing_times']) * 1000
            std_frame_time = np.std(benchmark_data['frame_processing_times']) * 1000
            print(f"   å¸§å¤„ç†æ—¶é—´: {avg_frame_time:.1f}Â±{std_frame_time:.1f}ms")
        
        if benchmark_data['confidence_scores']:
            avg_confidence = np.mean(benchmark_data['confidence_scores'])
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        
        if benchmark_data['bbox_stability']:
            avg_stability = np.mean(benchmark_data['bbox_stability'])
            print(f"   è·Ÿè¸ªç¨³å®šæ€§: {avg_stability:.1f}åƒç´ åç§»")
    
    # æ€§èƒ½ç­‰çº§è¯„ä¼°
    fps = tracker_stats.get('avg_fps', 0)
    success_rate = tracker_stats.get('success_rate', 0)
    
    if fps >= 60 and success_rate >= 90:
        grade = "ğŸ† ä¼˜ç§€"
    elif fps >= 30 and success_rate >= 80:
        grade = "âœ… è‰¯å¥½"
    elif fps >= 15 and success_rate >= 60:
        grade = "âš ï¸  ä¸€èˆ¬"
    else:
        grade = "âŒ éœ€è¦ä¼˜åŒ–"
    
    print(f"\nğŸ–ï¸  æ€§èƒ½è¯„çº§: {grade}")


def run_benchmark_suite(video_path, init_bbox):
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    print("\nğŸ§ª è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶...")
    
    test_configs = [
        {'frame_skip': 1, 'target_fps': 30, 'name': 'æ ‡å‡†é…ç½®'},
        {'frame_skip': 2, 'target_fps': 30, 'name': '2å€è·³å¸§'},
        {'frame_skip': 3, 'target_fps': 30, 'name': '3å€è·³å¸§'},
        {'frame_skip': 1, 'target_fps': 60, 'name': 'é«˜FPS'},
        {'frame_skip': 2, 'target_fps': 60, 'name': 'é«˜FPS+è·³å¸§'},
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\næµ‹è¯•é…ç½®: {config['name']}")
        try:
            output_path = f"benchmark_{config['name'].replace(' ', '_')}.mp4"
            _, stats = track_video_improved(
                video_path, init_bbox, output_path, 
                display=False, 
                frame_skip=config['frame_skip'],
                target_fps=config['target_fps'],
                benchmark=True
            )
            
            results.append({
                'config': config,
                'stats': stats,
                'output': output_path
            })
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    # æ‰“å°åŸºå‡†æµ‹è¯•æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ† åŸºå‡†æµ‹è¯•æ€»ç»“")
    print("="*80)
    print(f"{'é…ç½®':<15} {'FPS':<10} {'æˆåŠŸç‡':<10} {'å¤„ç†å¸§':<10} {'è·³å¸§æ•ˆç‡':<10}")
    print("-" * 80)
    
    for result in results:
        config = result['config']
        stats = result['stats']
        
        fps = stats.get('avg_fps', 0)
        success_rate = stats.get('success_rate', 0)
        processed = stats.get('processed_frames', 0)
        total = stats.get('total_frames', 1)
        skip_eff = (total - processed) / total * 100
        
        print(f"{config['name']:<15} {fps:<10.1f} {success_rate:<10.1f}% "
              f"{processed:<10} {skip_eff:<10.1f}%")
    
    # æ¸…ç†åŸºå‡†æµ‹è¯•æ–‡ä»¶
    for result in results:
        try:
            os.remove(result['output'])
        except:
            pass


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    if not os.path.exists(args.video):
        print(f"âŒ é”™è¯¯: è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video}")
        return 1
    
    print("ğŸš€ LightTrack æ”¹è¿›ç‰ˆå‘½ä»¤è¡Œè·Ÿè¸ªå·¥å…·")
    print("="*50)
    
    # è·å–åˆå§‹è¾¹ç•Œæ¡†
    if args.bbox:
        try:
            init_bbox = parse_bbox(args.bbox)
        except ValueError as e:
            print(f"âŒ é”™è¯¯: {e}")
            return 1
    else:
        print("æœªæŒ‡å®šè¾¹ç•Œæ¡†ï¼Œå°†è¿›å…¥äº¤äº’å¼é€‰æ‹©æ¨¡å¼")
        try:
            init_bbox = select_bbox_interactive(args.video)
            if init_bbox is None:
                print("æœªé€‰æ‹©ç›®æ ‡ï¼Œé€€å‡ºç¨‹åº")
                return 1
        except Exception as e:
            print(f"âŒ ç›®æ ‡é€‰æ‹©å¤±è´¥: {e}")
            return 1
    
    # åˆ†æè§†é¢‘å¹¶ç»™å‡ºå»ºè®®
    if args.auto_optimize:
        video_info, suggestions = analyze_video(args.video)
        if video_info and suggestions:
            print_video_analysis(video_info, suggestions)
            
            # åº”ç”¨å»ºè®®
            frame_skip = suggestions['frame_skip']
            target_fps = suggestions['target_fps']
            print(f"\nğŸ”§ åº”ç”¨è‡ªåŠ¨ä¼˜åŒ–å‚æ•°: è·³å¸§={frame_skip}, FPS={target_fps}")
        else:
            frame_skip = args.frame_skip
            target_fps = args.target_fps
    else:
        frame_skip = args.frame_skip
        target_fps = args.target_fps
    
    # åŸºå‡†æµ‹è¯•æ¨¡å¼
    if args.benchmark:
        run_benchmark_suite(args.video, init_bbox)
        return 0
    
    # å¼€å§‹è·Ÿè¸ª
    try:
        output_path, final_stats = track_video_improved(
            args.video, 
            init_bbox, 
            args.output, 
            args.display,
            frame_skip,
            target_fps,
            benchmark=False
        )
        
        print(f"\nâœ… å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_path}")
        return 0
        
    except Exception as e:
        print(f"âŒ è·Ÿè¸ªå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit(main())